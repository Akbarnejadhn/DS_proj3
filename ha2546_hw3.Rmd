---
title: "Methods of Classification"
author: "Hana Akbarnejad"
date: "4/10/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(viridis)
library(ggplot2)
library(readr)
library(patchwork)

library(caret)
library(glmnet)
library(MASS)
library(pROC)
library(AppliedPredictiveModeling)
library(ISLR)



knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


\newpage

```{r data, include=FALSE}

data(Weekly)
weekly_data = Weekly
head(weekly_data)

weekly_data = weekly_data %>%
  janitor::clean_names() %>% 
  dplyr::select(direction, everything(), -today) %>%   # not including year (?)
  mutate(direction = as.factor(direction))
```

In this exercise, I have been working with *Weekly* dataset from *ISLR* package. The dataset contains data about Weekly percentage returns for the S&P 500 stock index between 1990 and 2010 and it has `r nrow(weekly_data)` rowns and `r ncol(weekly_data)` columns. We consider a binary outcome (*direction*) with two levels: Up and Down, 5 *lag* variables (1 to 5), *year*, and *volume* as predictors. 

###  Part (a)

In this part, I included a graphical summary of the *Weekly* data:

```{r eda, echo=FALSE}

theme1 = transparentTheme(trans = .4)
theme1$strip.background$col = rgb(.0, .6, .2, .2)
trellis.par.set(theme1)
featurePlot(x = weekly_data[, 2:8],
y = weekly_data$direction,
scales = list(x=list(relation="free"),
y=list(relation="free")),
plot = "density", pch = "|",
auto.key = list(columns = 2))
```

The above plots plot marginal density functions within each response class for each predictor. We can observe that Up and Down classes of response have similar denisty functions for each predictor and there are a lot of overlaps within blue and red curves. Also, we can see that lag variables are almost normally distributed and we can observe positive skewness in volume variable. 

### Part (b)

In this part, I used the full dataset to perform a logistic regression with *direction* as the response and the five *Lag* variables and *Volume* as predictors.

```{r logistic_reg, echo=FALSE}

set.seed(2020)

ctrl = trainControl(method = "repeatedcv",
                    repeats = 5,
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)

glm_fit = train(x = weekly_data[,3:8],   # using lag variables and volume as predictors
                y = weekly_data$direction,
                method = "glm",
                metric = "ROC",
                trControl = ctrl)

summary(glm_fit)
```

Using all datset, the summary of *Glm* model shows that *lag2* with p-value of 0.03 is the only significant variable at 95% significance level.

### Part (c)

In this part, I will take a look at the confusion matrix and what it tells us about the model.

To do so, I have partitioned data into training and test (2/3-1/3), and refitted glm model on same variables as part (b) which is lag variables and volume. 

```{r confusion_matrix, echo=FALSE}

zero_one = contrasts(weekly_data$direction) # to know what is 0 and what is 1>>> 0:down, 1:up

# partitioning data into train and test
train_rows = createDataPartition(y = weekly_data$direction,
                    p = 2/3,
                    list = FALSE)

train_data = weekly_data[train_rows,]
test_data = weekly_data[-train_rows,]

glm_fit_train = glm(direction ~ lag1+lag2+lag3+lag4+lag5+volume,
                   data = weekly_data,
                   subset = train_rows,
                   family = binomial)

test_pred_prob = predict(glm_fit_train, newdata = test_data,
                    type = "response")

test_pred = rep("Down", length(test_pred_prob))
test_pred[test_pred_prob>0.5] = "Up"

confusionMatrix(data = as.factor(test_pred),
                reference = weekly_data$direction[-train_rows],
                positive = "Up")
```

The confusion matrix provided above shows the accuracy of 0.54 which means that 54% of data ( with 95% CI of (0.48, 0.59)) is correctly classified which is not very high and it shows that Glm model is probably not the best model to be used here. We also have No Information Rate of 0.55 which is a feature of data and shows the largest proportion (max) of each class. Here this number shows the the proportion of *Up* observations because most of the observations are in that class. We have a large p-value which shows that this prediction is not very meaningful and the accuracy is not significantly larger than the no information rate. Kappa value shows consistency of predicted and observed values. The Kappa value here is -0.023 which shows that these two values are not in agreement and that the agreement is even worse than random and the prediction is not meaningful. We observe a very high sensitivity(0.93) which means we do not have many false negatives and a very low specificity(0.04) which means we have many false-positives in prediction using this model. Also we have PPV of 0.55 and NPV of 0.35. These results show that the classification model is not performing perfectly and it might be useful if we can consider other methods of classification.

### Part (d) 

In this part, I have plotted the ROC curve using the predicted probability from logistic regression:

```{r roc_curve, echo=FALSE}

roc_glm = roc(weekly_data$direction[-train_rows], test_pred_prob)
plot(roc_glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_glm), col = 4, add = TRUE)

glm_auc = roc_glm$auc
```

It can be observed that AUC is `r glm_auc`. We usually consider AUC of 0.8 or higher as good, so this value is pretty low and shows that the classification model is not performing very well.

### Part (e) 

In this part, I have fitted logistic regression model again, but this time using a training data period from 1990 to 2008, with Lag1 and Lag2 as the predictors. Then I plotted the ROC curve using the held out data (that is, the data from 2009 and 2010).
```{r, echo=FALSE}

train_year_data = weekly_data %>% 
  filter(year %in% (1990:2008))

test_year_data = weekly_data %>% 
  filter(year %in% 2009:2010)

glm_fit_train2 = glm(direction ~ lag1+lag2,
                   data = train_year_data,
                   family = binomial)

summary(glm_fit_train2)

test_pred_prob2 = predict(glm_fit_train2, newdata = test_year_data,
                    type = "response")

test_pred2 = rep("Down", length(test_pred_prob2))
test_pred2[test_pred_prob2>0.5] = "Up"

roc_glm2 = roc(test_year_data$direction, test_pred_prob2)
plot(roc_glm2, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_glm2), col = 4, add = TRUE)

roc_glm2_auc = roc_glm2$auc

#glm caret
set.seed(2020)

model_glm = train(x = train_year_data[,3:4],
                  y = train_year_data$direction,
                  method = "glm",
                  metric = "ROC",
                  trControl = ctrl)
```

Doing so, it can be observed that neither lag1 or lag2 are significant anymore when fitting glm. Also the ROC curve above shows AUC value of `r roc_glm2_auc`, which is also not very good when predicting the direction od 2009 and 2010 using data from 1990 to 2008. 

### Part (f)

In this part, I repeat part (e) using LDA and QDA methods instead of Glm, still using lag1 and lah2 as predictors.

**LDA**
```{r lda, echo=FALSE}

lda_fit = lda(direction~ lag1+lag2,
              data = train_year_data)
plot(lda_fit)

lda_pred = predict(lda_fit, newdata = test_year_data)
roc_lda = roc(test_year_data$direction, lda_pred$posterior[,2],
              levels = c("Down", "Up"))

plot(roc_lda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_lda), col = 4, add = TRUE)

lda_auc = roc_lda$auc

#LDA caret
set.seed(2020)

model_lda = train(x = train_year_data[,3:4],
                y = train_year_data$direction,
                method = "lda",
                metric = "ROC",
                trControl = ctrl)
```

As above ROC curve shows, we get AUC of `r lda_auc` using LDA, which is pretty similar to AUC value using Glm method.

**QDA**
```{r qda, echo=FALSE}

qda_fit = qda(direction~ lag1+lag2,
              data = train_year_data)

qda_pred = predict(qda_fit, newdata = test_year_data)
roc_qda = roc(test_year_data$direction, qda_pred$posterior[,2],
              levels = c("Down", "Up"))

plot(roc_qda, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc_qda), col = 4, add = TRUE)

qda_auc = roc_qda$auc


#QDA caret
set.seed(2020)

model_qda = train(x = train_year_data[,3:4],
                  y = train_year_data$direction,
                  method = "qda",
                  metric = "ROC",
                  trControl = ctrl)
```

As above ROC curve shows, we get the area under the curve of `r qda_auc` using QDA, which is pretty similar to AUC value using LDA and Glm methods.

### Part (g)

In this part, I repeated the procedure using KNN.
```{r knn, echo=FALSE}

#KNN caret
set.seed(2020)

model_knn = train(x = train_year_data[,3:4],
                  y = train_year_data$direction,
                  method = "knn",
                  preProcess = c("center","scale"),
                  tuneGrid = data.frame(k = seq(1,500,by=5)),
                  trControl = ctrl)
ggplot(model_knn) # why does it look like this
k_bestTune = model_knn$bestTune

```

To find the best tuning parameter, I used the grid from 1 to 600, however, we can observe that the plot above does not reach a steady state or a decreasing effect. Theoretically, I should extend the grid till I get the desired shape in the plot but when extending the grid to beyond 500, I get the error regarding the presence of too many ties in knn which shows that the method cannot perform well for the values beyond 500. So, I have to stop at the maximum value that the method can run (500) which gives me the tuning parameter of `r k_bestTune`.

### Model comparison and discussion
```{r, echo=FALSE}

res = resamples(list(GLM = model_glm,
                LDA = model_lda,
                QDA = model_qda,
                KNN = model_knn))
summary(res)
bwplot(res)
```

Comparing models built using *caret()* and training data, we can observe that **LDA** model has the best performance considering ROC value with the highest mean and median values. **GLM** model has ROC value which is very close to LDA value. However, the ROC values for other models do not have a considerable difference and are in a close range. We can conclude that the predictive ability of all these models are not very different from each other. The below ROC curve also compare performace of models using test data.

```{r, echo=FALSE, message=FALSE}

glm.pred = predict(model_glm, newdata = test_year_data, type = "prob")[,2]
lda.pred = predict(model_lda, newdata = test_year_data, type = "prob")[,2]
qda.pred = predict(model_qda, newdata = test_year_data, type = "prob")[,2]
knn.pred = predict(model_knn, newdata = test_year_data, type = "prob")[,2]


roc.glm = roc(test_year_data$direction, glm.pred)
roc.lda = roc(test_year_data$direction, lda.pred)
roc.qda = roc(test_year_data$direction, qda.pred)
roc.knn = roc(test_year_data$direction, knn.pred)


auc = c(roc.glm$auc[1], roc.lda$auc[1],
        roc.qda$auc[1], roc.knn$auc[1])

plot(roc.glm, legacy.axes = TRUE)
plot(roc.lda, col = 3, add = TRUE)
plot(roc.qda, col = 4, add = TRUE)
plot(roc.knn, col = 6, add = TRUE)

modelNames = c("glm","lda","qda","knn")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:6, lwd = 2)
```

